{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 求解惩罚线性回归\n",
    "\n",
    "\n",
    "### 理解最小角度回归于前向逐步回归的关系\n",
    "\n",
    "【续前】\n",
    "\n",
    "### 理解Glmnet算法\n",
    "\n",
    "参考了glmnet的原始论文：\n",
    "\n",
    "Regularization Paths for Generalized Linear Models via Coordinate Descent（基于坐标下降法的广义线性模型正规化路径）, Jerome Friedman, 2010.pdf  链接: https://pan.baidu.com/s/1kUM1NCF 密码: pf25\n",
    "\n",
    "为了理解**ElasticNet回归**的数值算法，我们先看一个更一般的优化问题:\n",
    "\n",
    "$$\\boldsymbol{\\beta}^* = \\underset{\\boldsymbol{\\beta}}{\\rm{argmin}} \\ J(\\boldsymbol{\\beta}) = \\underset{\\boldsymbol{\\beta}}{\\rm{argmin}} \\left\\{ f(\\boldsymbol{\\beta}) + (1-\\alpha) \\dfrac{\\lambda}{2}  {\\| \\boldsymbol{\\beta} \\|}^2 + \\alpha \\lambda  {\\| \\boldsymbol{\\beta} \\|}_1\\right\\}$$\n",
    "\n",
    "作为数值解，只要$f(\\boldsymbol{\\beta})$一阶可导都不是问题，问题出在${\\| \\boldsymbol{\\beta} \\|}_1$是不连续的。我们注意到，${\\| \\boldsymbol{\\beta} \\|}_1$虽然不连续，但是其变量是解耦的（对回归系数元素的某个函数（比如绝对值）之和），而${\\| \\boldsymbol{\\beta} \\|}_2$也已经是解耦的。如果我们也能把$f(\\boldsymbol{\\beta})$解耦，那就方便了。当然直接这样做肯定是不可能的，但我们仅仅是求数值解，可以通过迭代得到最优值，这就给了我们对$f(\\boldsymbol{\\beta})$进行局部近似的机会。\n",
    "\n",
    "关于解耦，数学上有一个著名的不等式-**利普希茨(Lipschitz)条件**,存在常数$L>0$，使得:\n",
    "\n",
    "$${\\| f(\\boldsymbol{x}') - f(\\boldsymbol{x}) \\|}_2^2 \\le L {\\| \\boldsymbol{x}' - \\boldsymbol{x} \\|}^2 $$\n",
    "\n",
    "我们注意到不等式的右边恰好就是解耦的。这个条件是一个比一致连续更强的光滑条件，一般而言在优化问题中这个条件是满足的，比如**套索回归**和**ElasticNet回归**就满足（当然已解耦的部分${\\| \\boldsymbol{\\beta} \\|}_2$和${\\| \\boldsymbol{\\beta} \\|}_1$要剔除在外）。\n",
    "\n",
    "作为数值解，我们是通过迭代获取最优解，那么关于迭代过程中相邻的两个解$(\\boldsymbol{\\beta},\\tilde{\\boldsymbol{\\beta}})$可以对$f(\\boldsymbol{\\beta})$进行泰勒展开，并在二阶展开项利用**利普希茨(Lipschitz)条件**得到：\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "& f(\\boldsymbol{\\beta}) &\\simeq& f(\\tilde{\\boldsymbol{\\beta}}) + (\\nabla f(\\tilde{\\boldsymbol{\\beta}}))^T (\\boldsymbol{\\beta} - \\tilde{\\boldsymbol{\\beta}}) + \\dfrac{L}{2} {\\|\\boldsymbol{\\beta} - \\tilde{\\boldsymbol{\\beta}}\\|}^2 \\\\\n",
    "&=& \\dfrac{L}{2} {\\|\\boldsymbol{\\beta}-(\\tilde{\\boldsymbol{\\beta}} - \\dfrac{1}{L} \\nabla f(\\tilde{\\boldsymbol{\\beta}}))\\|}^2 + const \\\\\n",
    "&=& \\dfrac{L}{2} {\\|\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{z}} \\|}^2 + const, \\quad \\tilde{\\boldsymbol{z}} =\\tilde{\\boldsymbol{\\beta}} - \\dfrac{1}{L} \\nabla f(\\tilde{\\boldsymbol{\\beta}})\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "进而得到一个局部的，完全解耦的最优问题(同时将$L$吸收到$\\lambda$中去)：\n",
    "\n",
    "$$\\tilde{\\boldsymbol{\\beta}} \\leftarrow \\underset{\\boldsymbol{\\beta}}{\\rm{argmin}} \\left\\{\\dfrac{1}{2} {\\|\\boldsymbol{\\beta}-\\tilde{\\boldsymbol{z}} \\|}^2 + (1-\\alpha) \\dfrac{\\lambda}{2}  {\\| \\boldsymbol{\\beta} \\|}^2 + \\alpha \\lambda  {\\| \\boldsymbol{\\beta} \\|}_1\\right\\}, \\quad \\tilde{\\boldsymbol{z}} = \\tilde{\\boldsymbol{\\beta}} - \\nabla f(\\tilde{\\boldsymbol{\\beta}})$$\n",
    "\n",
    "对应的目标函数的一阶导数(可以分段求导)都是完全解耦的方程组，进而可以得到一个封闭解：\n",
    "\n",
    "$$ \\tilde{\\beta}_j \\leftarrow \\dfrac{\\tilde{z}_j - \\lambda \\alpha \\dfrac{\\partial}{\\partial \\tilde{\\beta}_j} |\\tilde{\\beta}_j|}{1+\\lambda (1-\\alpha)} , \\quad \\tilde{z}_j = \\tilde{\\beta}_j - (\\nabla f(\\tilde{\\boldsymbol{\\beta}}))_j, \\quad j=1,\\dots,n$$\n",
    "\n",
    "可以写成分段形式$S(z,g)$，称之为**套索系数的缩减函数**：\n",
    "\n",
    "$$\n",
    "S(z,g) = \\left\\{\n",
    "\\begin{array} \n",
    "&z-g, &z > g \\\\\n",
    "0, &|z| \\le g\\\\\n",
    "z+g, &z < -g\n",
    "\\end{array} \n",
    "\\right. \\quad g>0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\beta}_j \\leftarrow \\dfrac{S(\\tilde{z}_j,\\lambda \\alpha)}{1+ \\lambda (1-\\alpha)}\n",
    "$$\n",
    "\n",
    "特别地，**ElasticNet线性回归**而言：\n",
    "\n",
    "$$\n",
    "\\tilde{z}_j = \\tilde{\\beta}_j - (\\nabla f(\\tilde{\\boldsymbol{\\beta}}))_j = \\tilde{\\beta}_j + \\dfrac{1}{m} \\boldsymbol{x}_j^T (\\boldsymbol{y} - \\boldsymbol{X} \\tilde{\\boldsymbol{\\beta}}), \\quad \\boldsymbol{x}_j = \\boldsymbol{X}(:,j)\n",
    "$$\n",
    "\n",
    "进而可写出**ElasticNet递归公式**：\n",
    "\n",
    "$$\n",
    "\\tilde{\\beta}_j \\leftarrow \\dfrac{S(\\tilde{\\beta}_j + \\frac{1}{m} \\boldsymbol{x}_j^T(\\boldsymbol{y}-\\boldsymbol{X}\\tilde{\\boldsymbol{\\beta}}),\\lambda \\alpha)}{1+ \\lambda (1-\\alpha)}\n",
    "$$\n",
    "\n",
    "再特别一点$\\alpha=1$，就是**套索递归公式**：\n",
    "\n",
    "$$\\tilde{\\beta}_j \\leftarrow S(\\tilde{\\beta}_j + \\dfrac{1}{m} \\boldsymbol{x}_j^T(\\boldsymbol{y}-\\boldsymbol{X}\\tilde{\\boldsymbol{\\beta}}),\\lambda)$$\n",
    "\n",
    "\n",
    "**ElasticNet递归公式**对应的算法就是**Glmnet算法**。\n",
    "\n",
    "【注意】，**ElasticNet递归公式**写成了分量和向量形式的混杂， 这意味着：在计算过程中回归系数$\\tilde{\\beta}_j$会立刻更新（即，下一个回归系数$\\tilde{\\beta}_{j+1}$会用到已经更新的$\\tilde{\\beta}_k, k\\le j$），这种更新方式称为**异步更新**。另一种更新模式是**同步更新**，这种方式的递推公式可以写成全向量形式。\n",
    "\n",
    "下面依然是手写的代码实现（没用算法包）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def S(z, gamma):\n",
    "    '''\n",
    "        套索系数的缩减函数\n",
    "    '''\n",
    "    if gamma>=abs(z):\n",
    "        return 0\n",
    "    return (z/abs(z))*(abs(z)-gamma)\n",
    "\n",
    "def elasticNetRecursive(xx, y, beta, lam, alp):\n",
    "    '''\n",
    "        ElasticNet单步递归\n",
    "    参数：\n",
    "        xx:  属性矩阵X\n",
    "        y:   标签向量\n",
    "        beta: 回归系数向量(更新后返回)\n",
    "        lam:  \\lambda\n",
    "        alp:  \\alpha\n",
    "    '''\n",
    "    m,n = xx.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
